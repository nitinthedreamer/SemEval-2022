{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7BN39SSPFIB"
      },
      "source": [
        "Configuring PyTorch to use GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BBgjeQNZ91o4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41102246-d35d-4627-cd1e-4aa243d81fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "ROOT = '/content/drive'\n",
        "drive.mount(ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OEi-pbqsPMjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51dfdb1d-ef6b-41c4-bd31-760ab1725fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w84JACODIY-V"
      },
      "outputs": [],
      "source": [
        "from os.path import join  \n",
        "\n",
        "# path to your project on Google Drive\n",
        "MY_GOOGLE_DRIVE_PATH = 'MyDrive/Github/' \n",
        "# replace with your Github username \n",
        "GIT_USERNAME = \"YOUR_USERNAME\" \n",
        "# definitely replace with your\n",
        "GIT_TOKEN = \"YOUR_TOKEN\"  \n",
        "# Replace with your github repository in this case we want \n",
        "GIT_REPOSITORY = \"SemEval-2022\" \n",
        "\n",
        "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "!mkdir \"{PROJECT_PATH}\"    \n",
        "GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "print(\"GIT_PATH: \", GIT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRgpi9A9MsNf"
      },
      "outputs": [],
      "source": [
        "%cd \"{PROJECT_PATH}\"\n",
        "!git clone \"{GIT_PATH}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FetT4jK3PO2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a26cd92-58d8-4b8b-cf22-4c6b0412cc74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.8.0\n",
            "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
            "\u001b[K     |████████████████████████████████| 563 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 20.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 52.7 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.20.22-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 24.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.22\n",
            "  Downloading botocore-1.23.22-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 54.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.22->boto3->transformers==2.8.0) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.22->boto3->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.1.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sentencepiece, sacremoses, boto3, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.22 botocore-1.23.22 jmespath-0.10.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.5.2 transformers-2.8.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers=='2.8.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GVcHxFqNL5Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109f19c0-0aaf-44f5-ecaa-f0a8479afd0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/SemEval-2022/pcl\n"
          ]
        }
      ],
      "source": [
        "import sys  \n",
        "sys.path.insert(0, '/content/drive/MyDrive/Github/SemEval-2022/pcl')\n",
        "%cd /content/drive/MyDrive/Github/SemEval-2022/pcl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DecQES1fMC_c"
      },
      "outputs": [],
      "source": [
        "from dont_patronize_me import DontPatronizeMe\n",
        "\n",
        "# Initialize a dpm (Don't Patronize Me) object.\n",
        "# It takes two areguments as input: \n",
        "# (1) Path to the directory containing the training set files, which is the root directory of this notebook.\n",
        "# (2) Path to the test set, which will be released when the evaluation phase begins. In this example, \n",
        "# we use the dataset for Subtask 1, which the code will load without labels.\n",
        "dpm = DontPatronizeMe('/content/drive/MyDrive/Github/SemEval-2022/dataset/pcl', '.dontpatronizeme_pcl.tsv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMHjcTqLMU3z"
      },
      "source": [
        "## Load Subtask 1 Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3BfPChVjMZx3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e45452f0-e7d8-4aae-c93d-6b1c078a842a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>par_id</th>\n",
              "      <th>art_id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>country</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>orig_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>@@24942188</td>\n",
              "      <td>hopeless</td>\n",
              "      <td>ph</td>\n",
              "      <td>we 're living in times of absolute insanity , ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>@@21968160</td>\n",
              "      <td>migrant</td>\n",
              "      <td>gh</td>\n",
              "      <td>in libya today , there are countless number of...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>@@16584954</td>\n",
              "      <td>immigrant</td>\n",
              "      <td>ie</td>\n",
              "      <td>\"white house press secretary sean spicer said ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>@@7811231</td>\n",
              "      <td>disabled</td>\n",
              "      <td>nz</td>\n",
              "      <td>council customers only signs would be displaye...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>@@1494111</td>\n",
              "      <td>refugee</td>\n",
              "      <td>ca</td>\n",
              "      <td>\"\"\" just like we received migrants fleeing el ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  par_id      art_id  ... label orig_label\n",
              "0      1  @@24942188  ...     0          0\n",
              "1      2  @@21968160  ...     0          0\n",
              "2      3  @@16584954  ...     0          0\n",
              "3      4   @@7811231  ...     0          0\n",
              "4      5   @@1494111  ...     0          0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# This method loads the subtask 1 data\n",
        "dpm.load_task1()\n",
        "# which we can then access as a dataframe\n",
        "dpm.train_task1_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xH9XjIlnQHQc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss, NLLLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, XLNetTokenizer, XLNetModel, TFXLNetModel, XLNetLMHeadModel, XLNetConfig, XLNetForSequenceClassification\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "X = dpm.train_task1_df['text']\n",
        "y = dpm.train_task1_df['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Text"
      ],
      "metadata": {
        "id": "xiBfXwjc1bxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import itertools\n",
        "\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "\n",
        "def text_cleaner(text):\n",
        "    # removing UTF-8 BOM (Byte Order Mark)\n",
        "    try:\n",
        "        text = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\") # The UTF-8 BOM is a sequence of bytes (EF BB BF) that allows the reader to identify a file as being encoded in UTF-8\n",
        "    except:\n",
        "        text = text\n",
        "    \n",
        "    \n",
        "    #replace consecutive non-ASCII characters with a space\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
        "    \n",
        "    # HTML encoding\n",
        "    soup = BeautifulSoup(text, 'lxml') #HTML encoding has not been converted to text, and ended up in text field as ‘&amp’,’&quot’,etc.\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    # Removing URLs\n",
        "    text = re.sub(pat2, '', text)\n",
        "    \n",
        "    # Removing punctuations\n",
        "    text = re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\(\\)\\[\\]\\\"\\%\\*\\#\\@]\", \" \", text)\n",
        "    text = re.sub(\"\\s\\'\\s\", \" \", text)\n",
        "    \n",
        "    # Expanding/Collapsing punctuated words\n",
        "    text = re.sub(\"\\s\\'\", \"\\'\", text)\n",
        "    text = re.sub(\"n\\'t\", \"not\", text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    # Fix misspelled words\n",
        "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))# checking that each character should occur not more than 2 times in every word\n",
        "\n",
        "    # Remove leading and trailing whitespaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Change to lowercase\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "5Q118N8I0rMg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = list(map(text_cleaner, X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsZ0zPhs1kxB",
        "outputId": "bc38f00d-5190-4a26-b03e-5d9aed5b5002"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skin needling is a safe and precise way to traumatise the skin to stimulate collagen , based on the above principal . it results in a smoother , firmer and younger looking skin .\n",
            "skin needling is a safe and precise way to traumatise the skin to stimulate collagen based on the above principal it results in a smoother firmer and younger looking skin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KLTMWzcXrtF"
      },
      "source": [
        "To feed our text to XLNet, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "927fWd0cVRza"
      },
      "outputs": [],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\", do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLWWGEH2XzlQ"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "XLNet requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
        "\n",
        "    input ids: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary\n",
        "    segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
        "    attention mask: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens\n",
        "    labels: a single value of 1 or 0. In our task 1 means “PCL” and 0 means “No PCL”\n",
        "\n",
        "Although we can have variable length input sentences, XLNet does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length.\n",
        "\n",
        "To “pad” our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n",
        "\n",
        "If a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n",
        "\n",
        "We pad and truncate our sequences so that they all become of length maxlen (“post” indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) pad_sequences is a utility function that we’re borrowing from Keras. It simply handles the truncating and padding of Python lists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBCcj21jWtXR"
      },
      "outputs": [],
      "source": [
        "def tokenize_inputs(text_list, tokenizer, num_embeddings=120):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text input into ids. Appends the appropriate special\n",
        "    characters to the end of the text to denote end of sentence. Truncate or pad\n",
        "    the appropriate sequence length.\n",
        "    \"\"\"\n",
        "    # tokenize the text, then truncate sequence to the desired length minus 2 for\n",
        "    # the 2 special characters\n",
        "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n",
        "    # convert tokenized text into numeric ids for the appropriate LM\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    # append special token \"<s>\" and </s> to end of sentence\n",
        "    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n",
        "    # pad sequences\n",
        "    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    return input_ids\n",
        "\n",
        "def create_attn_masks(input_ids):\n",
        "    \"\"\"\n",
        "    Create attention masks to tell model whether attention should be applied to\n",
        "    the input id tokens. Do not want to perform attention on padding tokens.\n",
        "    \"\"\"\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aczK5b6uW314"
      },
      "outputs": [],
      "source": [
        "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "\n",
        "input_ids = tokenize_inputs(X, tokenizer, num_embeddings=120)\n",
        "attention_masks = create_attn_masks(input_ids)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.from_numpy(input_ids)\n",
        "attention_masks = torch.tensor(attention_masks)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', X[1])\n",
        "print('Token IDs:', input_ids[1])\n",
        "print('Attention Masks:', attention_masks[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIg7ZH6Hbkmi"
      },
      "source": [
        "## Generating Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9HaedMyZdhs"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, y)\n",
        "\n",
        "# Create a 75-15-10 train-validation-test split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.75 * len(dataset))\n",
        "val_size = round(0.6*(len(dataset) - train_size))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "print(len(dataset))\n",
        "print(train_size)\n",
        "print(val_size)\n",
        "print(test_size)\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHv4pp0gcS-B"
      },
      "source": [
        "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihM97krecTzW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. Batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2JFDgQJHfnl"
      },
      "source": [
        "# Load pretrained XLNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYkqcfO_Hgfi"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetForSequenceClassification\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72elEeg3FOtK"
      },
      "source": [
        "# Setting up Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAlZr8xTFXIl"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
        "\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "XLNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}