{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XLNet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bf100589c2fe4bf29915e767d2daec72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2d854ee2c4b94412947f2577e9b1b76a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e6cdbae8dea349589e984f01a7eba990","IPY_MODEL_60953e669a334846af35aa5925b70985","IPY_MODEL_d3462fee46244843ba2920ed881a8687"]}},"2d854ee2c4b94412947f2577e9b1b76a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6cdbae8dea349589e984f01a7eba990":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_26c5bc1e61bb437283797dfa305a683e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9557bf1574454d43ab14a68ab5611c02"}},"60953e669a334846af35aa5925b70985":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_af849fc01fa348d093921e01e5159850","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":798011,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":798011,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f74ee03988f54b4997a415a848f661cc"}},"d3462fee46244843ba2920ed881a8687":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e0c542de815142d5bf832203487e3e8e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 798k/798k [00:00&lt;00:00, 686kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd89a84526dc409a977761486eb97fb5"}},"26c5bc1e61bb437283797dfa305a683e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9557bf1574454d43ab14a68ab5611c02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af849fc01fa348d093921e01e5159850":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f74ee03988f54b4997a415a848f661cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0c542de815142d5bf832203487e3e8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dd89a84526dc409a977761486eb97fb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","source":["Configuring PyTorch to use GPU"],"metadata":{"id":"I7BN39SSPFIB"}},{"cell_type":"code","source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"id":"OEi-pbqsPMjA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638981254549,"user_tz":420,"elapsed":359,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}},"outputId":"68a90f3b-8858-43a0-8d17-ea8213773c0c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"]}]},{"cell_type":"code","source":["!pip install transformers=='2.8.0'"],"metadata":{"id":"FetT4jK3PO2N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638981258209,"user_tz":420,"elapsed":3220,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}},"outputId":"94b74c27-3497-4efd-f860-31ef3c9bf0e5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.4.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.20.21)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.1.96)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.62.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.5.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.0.46)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (0.10.0)\n","Requirement already satisfied: botocore<1.24.0,>=1.23.21 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (1.23.21)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (0.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.21->boto3->transformers==2.8.0) (1.25.11)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.21->boto3->transformers==2.8.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.21->boto3->transformers==2.8.0) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.1.0)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBgjeQNZ91o4","executionInfo":{"status":"ok","timestamp":1638982523239,"user_tz":420,"elapsed":1137,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}},"outputId":"39951f25-d3ab-4020-dd37-48f59e1776e4"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import sys  \n","sys.path.insert(0, '/content/drive/MyDrive/pcl/')\n","%cd /content/drive/MyDrive/pcl/\n","%pwd"],"metadata":{"id":"GVcHxFqNL5Nb","executionInfo":{"status":"ok","timestamp":1638983106795,"user_tz":420,"elapsed":469,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}},"colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"35b7402a-2d58-443d-a976-5ee231766997"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1URsFCKbzbYU73LsLYjXrV3HIIa6_jysM/pcl\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/.shortcut-targets-by-id/1URsFCKbzbYU73LsLYjXrV3HIIa6_jysM/pcl'"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["from dont_patronize_me import DontPatronizeMe\n","\n","# Initialize a dpm (Don't Patronize Me) object.\n","# It takes two areguments as input: \n","# (1) Path to the directory containing the training set files, which is the root directory of this notebook.\n","# (2) Path to the test set, which will be released when the evaluation phase begins. In this example, \n","# we use the dataset for Subtask 1, which the code will load without labels.\n","dpm = DontPatronizeMe('/content/drive/MyDrive/pcl/dataset/', '.dontpatronizeme_pcl.tsv')"],"metadata":{"id":"DecQES1fMC_c","executionInfo":{"status":"ok","timestamp":1638983113781,"user_tz":420,"elapsed":1413,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["## Load Subtask 1 Data"],"metadata":{"id":"AMHjcTqLMU3z"}},{"cell_type":"code","source":["# This method loads the subtask 1 data\n","dpm.load_task1()\n","# which we can then access as a dataframe\n","dpm.train_task1_df.head()"],"metadata":{"id":"3BfPChVjMZx3","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1638983124369,"user_tz":420,"elapsed":2333,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}},"outputId":"a95c9057-ad60-4bab-a92f-7c91e6d5c7bc"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>par_id</th>\n","      <th>art_id</th>\n","      <th>keyword</th>\n","      <th>country</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>orig_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>@@24942188</td>\n","      <td>hopeless</td>\n","      <td>ph</td>\n","      <td>we 're living in times of absolute insanity , ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>@@21968160</td>\n","      <td>migrant</td>\n","      <td>gh</td>\n","      <td>in libya today , there are countless number of...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>@@16584954</td>\n","      <td>immigrant</td>\n","      <td>ie</td>\n","      <td>\"white house press secretary sean spicer said ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>@@7811231</td>\n","      <td>disabled</td>\n","      <td>nz</td>\n","      <td>council customers only signs would be displaye...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>@@1494111</td>\n","      <td>refugee</td>\n","      <td>ca</td>\n","      <td>\"\"\" just like we received migrants fleeing el ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  par_id      art_id  ... label orig_label\n","0      1  @@24942188  ...     0          0\n","1      2  @@21968160  ...     0          0\n","2      3  @@16584954  ...     0          0\n","3      4   @@7811231  ...     0          0\n","4      5   @@1494111  ...     0          0\n","\n","[5 rows x 7 columns]"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import os\n","import math\n","import torch\n","from torch.nn import BCEWithLogitsLoss, NLLLoss\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, XLNetTokenizer, XLNetModel, TFXLNetModel, XLNetLMHeadModel, XLNetConfig, XLNetForSequenceClassification\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from tqdm import tqdm, trange\n","\n","X = dpm.train_task1_df['text']\n","y = dpm.train_task1_df['label']\n"],"metadata":{"id":"xH9XjIlnQHQc","executionInfo":{"status":"ok","timestamp":1638983154355,"user_tz":420,"elapsed":6991,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["To feed our text to XLNet, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary."],"metadata":{"id":"6KLTMWzcXrtF"}},{"cell_type":"code","source":["tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\", do_lower_case=True)"],"metadata":{"id":"927fWd0cVRza","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["bf100589c2fe4bf29915e767d2daec72","2d854ee2c4b94412947f2577e9b1b76a","e6cdbae8dea349589e984f01a7eba990","60953e669a334846af35aa5925b70985","d3462fee46244843ba2920ed881a8687","26c5bc1e61bb437283797dfa305a683e","9557bf1574454d43ab14a68ab5611c02","af849fc01fa348d093921e01e5159850","f74ee03988f54b4997a415a848f661cc","e0c542de815142d5bf832203487e3e8e","dd89a84526dc409a977761486eb97fb5"]},"executionInfo":{"status":"ok","timestamp":1638983157670,"user_tz":420,"elapsed":2485,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}},"outputId":"e23c496d-c542-4ef0-b470-839107f4b884"},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf100589c2fe4bf29915e767d2daec72","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Tokenization\n","\n","XLNet requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n","\n","    input ids: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary\n","    segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n","    attention mask: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens\n","    labels: a single value of 1 or 0. In our task 1 means “PCL” and 0 means “No PCL”\n","\n","Although we can have variable length input sentences, XLNet does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length.\n","\n","To “pad” our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n","\n","If a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n","\n","We pad and truncate our sequences so that they all become of length maxlen (“post” indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) pad_sequences is a utility function that we’re borrowing from Keras. It simply handles the truncating and padding of Python lists.\n"],"metadata":{"id":"DLWWGEH2XzlQ"}},{"cell_type":"code","source":["def tokenize_inputs(text_list, tokenizer, num_embeddings=120):\n","    \"\"\"\n","    Tokenizes the input text input into ids. Appends the appropriate special\n","    characters to the end of the text to denote end of sentence. Truncate or pad\n","    the appropriate sequence length.\n","    \"\"\"\n","    # tokenize the text, then truncate sequence to the desired length minus 2 for\n","    # the 2 special characters\n","    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n","    # convert tokenized text into numeric ids for the appropriate LM\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    # append special token \"<s>\" and </s> to end of sentence\n","    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n","    # pad sequences\n","    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    return input_ids\n","\n","def create_attn_masks(input_ids):\n","    \"\"\"\n","    Create attention masks to tell model whether attention should be applied to\n","    the input id tokens. Do not want to perform attention on padding tokens.\n","    \"\"\"\n","    # Create attention masks\n","    attention_masks = []\n","\n","    # Create a mask of 1s for each token followed by 0s for padding\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return attention_masks"],"metadata":{"id":"WBCcj21jWtXR","executionInfo":{"status":"ok","timestamp":1638983180659,"user_tz":420,"elapsed":259,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to their word IDs.\n","\n","input_ids = tokenize_inputs(X, tokenizer, num_embeddings=120)\n","attention_masks = create_attn_masks(input_ids)\n","\n","# Convert the lists into tensors.\n","input_ids = torch.from_numpy(input_ids)\n","attention_masks = torch.tensor(attention_masks)\n","y = torch.tensor(y)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', X[1])\n","print('Token IDs:', input_ids[1])\n","print('Token IDs:', attention_masks[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aczK5b6uW314","executionInfo":{"status":"ok","timestamp":1638983191274,"user_tz":420,"elapsed":6806,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}},"outputId":"b832b970-fdc3-4c43-b500-524d1c840f70"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  in libya today , there are countless number of ghanaian and nigerian immigrants . these are the two countries with key macroeconomic challenges including unemployment . let 's tackle this issue from the root and not the fruit . thank you\n","Token IDs: tensor([   25,    17, 11684,  1489,   494,    17,    19,   105,    41, 13035,\n","          243,    20,    17, 20185,   101,   884,    21,    17,  1197,  2371,\n","          884,  4922,    17,     9,   166,    41,    18,    87,   452,    33,\n","          792, 30552,  3507,   208,  4658,    17,     9,   618,    17,    26,\n","           23,  6204,    52,   671,    40,    18,  5988,    21,    50,    18,\n","         3792,    17,     9,  4553,    44,     4,     3,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","Token IDs: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"]}]},{"cell_type":"markdown","source":["## Generating Data Splits"],"metadata":{"id":"aIg7ZH6Hbkmi"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, random_split\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, y)\n","\n","# Create a 75-15-10 train-validation-test split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.75 * len(dataset))\n","val_size = round(0.6*(len(dataset) - train_size))\n","test_size = len(dataset) - train_size - val_size\n","\n","print(len(dataset))\n","print(train_size)\n","print(val_size)\n","print(test_size)\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"],"metadata":{"id":"K9HaedMyZdhs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638983191274,"user_tz":420,"elapsed":8,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}},"outputId":"55f625bb-e822-46a6-f360-ec4f6dca0e40"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["10469\n","7851\n","1571\n","1047\n"]}]},{"cell_type":"markdown","source":["We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."],"metadata":{"id":"bHv4pp0gcS-B"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. Batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","test_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"metadata":{"id":"ihM97krecTzW","executionInfo":{"status":"ok","timestamp":1638983198799,"user_tz":420,"elapsed":437,"user":{"displayName":"Nitin Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02878659376118974946"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"b-9NpuBLAy-C"},"execution_count":null,"outputs":[]}]}