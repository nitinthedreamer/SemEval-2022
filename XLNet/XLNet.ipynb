{"cells":[{"cell_type":"markdown","metadata":{"id":"I7BN39SSPFIB"},"source":["Configuring PyTorch to use GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBgjeQNZ91o4"},"outputs":[],"source":["from google.colab import drive\n","ROOT = '/content/drive'\n","drive.mount(ROOT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEi-pbqsPMjA"},"outputs":[],"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w84JACODIY-V"},"outputs":[],"source":["from os.path import join  \n","\n","# path to your project on Google Drive\n","MY_GOOGLE_DRIVE_PATH = 'MyDrive/Github/' \n","# replace with your Github username \n","GIT_USERNAME = \"rohandas14\" \n","# definitely replace with your\n","GIT_TOKEN = \"YOUR_TOKEN\"  \n","# Replace with your github repository in this case we want \n","GIT_REPOSITORY = \"SemEval-2022\" \n","\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","!mkdir \"{PROJECT_PATH}\"    \n","GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n","print(\"GIT_PATH: \", GIT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRgpi9A9MsNf"},"outputs":[],"source":["%cd \"{PROJECT_PATH}\"\n","!git clone \"{GIT_PATH}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FetT4jK3PO2N"},"outputs":[],"source":["!pip install transformers=='2.8.0'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLo-s0RlNaOI"},"outputs":[],"source":["%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVcHxFqNL5Nb"},"outputs":[],"source":["import sys  \n","sys.path.insert(0, '/content/drive/MyDrive/Github/SemEval-2022/pcl')\n","%cd /content/drive/MyDrive/Github/SemEval-2022/pcl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DecQES1fMC_c"},"outputs":[],"source":["from dont_patronize_me import DontPatronizeMe\n","\n","# Initialize a dpm (Don't Patronize Me) object.\n","# It takes two areguments as input: \n","# (1) Path to the directory containing the training set files, which is the root directory of this notebook.\n","# (2) Path to the test set, which will be released when the evaluation phase begins. In this example, \n","# we use the dataset for Subtask 1, which the code will load without labels.\n","dpm = DontPatronizeMe('/content/drive/MyDrive/Github/SemEval-2022/dataset/', '.dontpatronizeme_pcl.tsv')"]},{"cell_type":"markdown","metadata":{"id":"AMHjcTqLMU3z"},"source":["## Load Subtask 1 Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BfPChVjMZx3"},"outputs":[],"source":["# This method loads the subtask 1 data\n","dpm.load_task1()\n","# which we can then access as a dataframe\n","dpm.train_task1_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xH9XjIlnQHQc"},"outputs":[],"source":["import pandas as pd\n","import re\n","import os\n","import math\n","import torch\n","from torch.nn import BCEWithLogitsLoss, NLLLoss\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, XLNetTokenizer, XLNetModel, TFXLNetModel, XLNetLMHeadModel, XLNetConfig, XLNetForSequenceClassification\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from tqdm import tqdm, trange\n","\n","X = dpm.train_task1_df['text']\n","y = dpm.train_task1_df['label']\n"]},{"cell_type":"markdown","metadata":{"id":"6KLTMWzcXrtF"},"source":["To feed our text to XLNet, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"927fWd0cVRza"},"outputs":[],"source":["tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\", do_lower_case=True)"]},{"cell_type":"markdown","metadata":{"id":"DLWWGEH2XzlQ"},"source":["## Tokenization\n","\n","XLNet requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n","\n","    input ids: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary\n","    segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n","    attention mask: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens\n","    labels: a single value of 1 or 0. In our task 1 means “PCL” and 0 means “No PCL”\n","\n","Although we can have variable length input sentences, XLNet does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length.\n","\n","To “pad” our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n","\n","If a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n","\n","We pad and truncate our sequences so that they all become of length maxlen (“post” indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) pad_sequences is a utility function that we’re borrowing from Keras. It simply handles the truncating and padding of Python lists.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBCcj21jWtXR"},"outputs":[],"source":["def tokenize_inputs(text_list, tokenizer, num_embeddings=120):\n","    \"\"\"\n","    Tokenizes the input text input into ids. Appends the appropriate special\n","    characters to the end of the text to denote end of sentence. Truncate or pad\n","    the appropriate sequence length.\n","    \"\"\"\n","    # tokenize the text, then truncate sequence to the desired length minus 2 for\n","    # the 2 special characters\n","    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n","    # convert tokenized text into numeric ids for the appropriate LM\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    # append special token \"<s>\" and </s> to end of sentence\n","    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n","    # pad sequences\n","    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    return input_ids\n","\n","def create_attn_masks(input_ids):\n","    \"\"\"\n","    Create attention masks to tell model whether attention should be applied to\n","    the input id tokens. Do not want to perform attention on padding tokens.\n","    \"\"\"\n","    # Create attention masks\n","    attention_masks = []\n","\n","    # Create a mask of 1s for each token followed by 0s for padding\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aczK5b6uW314"},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to their word IDs.\n","\n","input_ids = tokenize_inputs(X, tokenizer, num_embeddings=120)\n","attention_masks = create_attn_masks(input_ids)\n","\n","# Convert the lists into tensors.\n","input_ids = torch.from_numpy(input_ids)\n","attention_masks = torch.tensor(attention_masks)\n","y = torch.tensor(y)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', X[1])\n","print('Token IDs:', input_ids[1])\n","print('Attention Masks:', attention_masks[1])"]},{"cell_type":"markdown","metadata":{"id":"aIg7ZH6Hbkmi"},"source":["## Generating Data Splits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9HaedMyZdhs"},"outputs":[],"source":["from torch.utils.data import TensorDataset, random_split\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, y)\n","\n","# Create a 75-15-10 train-validation-test split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.75 * len(dataset))\n","val_size = round(0.6*(len(dataset) - train_size))\n","test_size = len(dataset) - train_size - val_size\n","\n","print(len(dataset))\n","print(train_size)\n","print(val_size)\n","print(test_size)\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"]},{"cell_type":"markdown","metadata":{"id":"bHv4pp0gcS-B"},"source":["We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihM97krecTzW"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. Batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","test_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"]},{"cell_type":"markdown","metadata":{"id":"w2JFDgQJHfnl"},"source":["# Load pretrained XLNet model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYkqcfO_Hgfi"},"outputs":[],"source":["from transformers import XLNetForSequenceClassification\n","model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"72elEeg3FOtK"},"source":["# Setting up Hyperparameters\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAlZr8xTFXIl"},"outputs":[],"source":["EPOCHS = 3\n","BATCH_SIZE = 32\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n","\n","total_steps = len(train_data_loader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"XLNet.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
